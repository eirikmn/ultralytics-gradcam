{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#plot a cv2 image\n",
    "import cv2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# add modified YOLOv8 library (cloned from git: URL)\n",
    "\n",
    "sys.path.append('ultralytics')\n",
    "from ultralytics import YOLO #make sure you do not have another library with the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " predict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor init \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " setting up predictor_model, passing model=self.model \n",
      " type(self.predictor)\n",
      "<class 'ultralytics.yolo.v8.detect.predict.DetectionPredictor'>\n",
      "\n",
      " type(self.model)\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor setup model \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "\n",
      " basepredictor setup model \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " hi autobackend \n",
      "\n",
      "\n",
      "\n",
      " autobackend receive nn.module \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " print type(self.model) \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " type(self.predictor)\n",
      " \n",
      "<class 'ultralytics.yolo.v8.detect.predict.DetectionPredictor'>\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor __call__ \n",
      "\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor stream_inference \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " req grad True \n",
      "\n",
      "\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      " req grad True1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " req grad True2 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch#\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch reqgrad\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      " autobackend forward \n",
      "\n",
      "\n",
      "b, ch, h, w \n",
      "\n",
      "1 3 1280 1280\n",
      "autobackend self.pt or self.nn_module \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "predict once\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.block.SPPF'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.head.Detect'>\n",
      "3\n",
      "\n",
      "\n",
      " autobackend forward y \n",
      "\n",
      "\n",
      "2\n",
      "list or tuple\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      " backpropprop \n",
      "\n",
      "\n",
      "/Users/emy016/Dropbox/Postdoc2/Kurs/NORA summer school 2023/NORAprosjekt/sampleimg-2.txt\n",
      "torch.Size([25, 6])\n",
      "25\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.0000, 2.0000, 0.2016, 0.1430, 0.0703, 0.1000])\n",
      "\n",
      "\n",
      " backpropprop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " targettensor \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " backprop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "v8DetectionLoss\n",
      "\n",
      "dict_keys(['task', 'mode', 'model', 'data', 'epochs', 'patience', 'batch', 'imgsz', 'save', 'save_period', 'cache', 'device', 'workers', 'project', 'name', 'exist_ok', 'pretrained', 'optimizer', 'verbose', 'seed', 'deterministic', 'single_cls', 'rect', 'cos_lr', 'close_mosaic', 'resume', 'amp', 'fraction', 'profile', 'overlap_mask', 'mask_ratio', 'dropout', 'val', 'split', 'save_json', 'save_hybrid', 'conf', 'iou', 'max_det', 'half', 'dnn', 'plots', 'source', 'show', 'save_txt', 'save_conf', 'save_crop', 'show_labels', 'show_conf', 'vid_stride', 'line_width', 'visualize', 'augment', 'agnostic_nms', 'classes', 'retina_masks', 'boxes', 'format', 'keras', 'optimize', 'int8', 'dynamic', 'simplify', 'opset', 'workspace', 'nms', 'lr0', 'lrf', 'momentum', 'weight_decay', 'warmup_epochs', 'warmup_momentum', 'warmup_bias_lr', 'box', 'cls', 'dfl', 'pose', 'kobj', 'label_smoothing', 'nbs', 'hsv_h', 'hsv_s', 'hsv_v', 'degrees', 'translate', 'scale', 'shear', 'perspective', 'flipud', 'fliplr', 'mosaic', 'mixup', 'copy_paste', 'cfg', 'v5loader', 'tracker'])\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['args', 'save_dir', 'done_warmup', 'model', 'data', 'imgsz', 'device', 'dataset', 'vid_path', 'vid_writer', 'plotted_img', 'data_path', 'source_type', 'batch', 'results', 'transforms', 'callbacks', 'stream', 'seen', 'windows'])\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'self', 'weights', 'device', 'dnn', 'data', 'fp16', 'fuse', 'verbose', 'w', 'nn_module', 'pt', 'jit', 'onnx', 'xml', 'engine', 'coreml', 'saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs', 'paddle', 'triton', 'nhwc', 'stride', 'model', 'metadata', 'cuda', 'names', '__class__'])\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'yaml', 'save', 'names', 'inplace', 'stride', 'nc', 'args', 'pt_path', 'task'])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " v8detpred2 -> type(self)\n",
      "\n",
      "\n",
      "\n",
      "pred_distri, pred_scores\n",
      "\n",
      "torch.Size([1, 68, 160, 160])\n",
      "1\n",
      "68\n",
      "64\n",
      "4\n",
      "\n",
      "\n",
      " pred_scores.shape \n",
      "\n",
      "\n",
      "torch.Size([1, 33600, 4])\n",
      "\n",
      "\n",
      " pred_scores.shape \n",
      "\n",
      "\n",
      "\n",
      " preprocess \n",
      "\n",
      "\n",
      "\n",
      " gt labels\n",
      "\n",
      "\n",
      "tensor([[[2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.],\n",
      "         [2.]]])\n",
      "torch.Size([1, 25, 1])\n",
      "tensor([[[ 213.0000,  119.0000,  303.0000,  247.0000],\n",
      "         [ 477.0000,   23.0000,  503.0000,   55.0000],\n",
      "         [ 375.0000,  118.5000,  397.0000,  143.5000],\n",
      "         [ 389.0000,  190.5000,  429.0000,  241.5000],\n",
      "         [ 483.5000,  195.0000,  512.5000,  225.0000],\n",
      "         [ 585.5000,   39.5000,  624.5000,   82.5000],\n",
      "         [ 560.5000,  156.5000,  607.5000,  203.5000],\n",
      "         [ 727.0000,  137.5000,  763.0000,  178.5000],\n",
      "         [ 432.5000,  363.0000,  477.5000,  411.0000],\n",
      "         [ 695.5000,  363.0000,  734.5000,  409.0000],\n",
      "         [ 798.5000,  361.0000,  819.5000,  391.0000],\n",
      "         [ 888.0000,  337.0000,  928.0000,  391.0000],\n",
      "         [ 370.5000,  572.5000,  405.5000,  613.5000],\n",
      "         [ 552.5000,  623.0000,  581.5000,  655.0000],\n",
      "         [ 678.5000,  543.5000,  743.5000,  578.5000],\n",
      "         [ 855.5000,  646.5000,  896.5000,  687.5000],\n",
      "         [ 811.5000,  710.5000,  902.5000,  799.5000],\n",
      "         [ 330.5000,  789.0000,  357.5000,  819.0000],\n",
      "         [ 442.5000,  851.5000,  589.5000,  968.5000],\n",
      "         [ 296.5000,  963.5000,  345.5000, 1002.5000],\n",
      "         [ 844.5000,  903.5000,  947.5000, 1010.5000],\n",
      "         [ 708.5000, 1000.5000,  743.5000, 1033.5000],\n",
      "         [ 253.5000, 1062.5000,  284.5000, 1101.5000],\n",
      "         [ 238.0000, 1116.0000,  422.0000, 1234.0000],\n",
      "         [ 486.5000, 1165.5000,  523.5000, 1198.5000]]])\n",
      "torch.Size([1, 25, 4])\n",
      "\n",
      "\n",
      " gt labels\n",
      "\n",
      "\n",
      "\n",
      " bbox_decode \n",
      "\n",
      "torch.Size([1, 33600, 4])\n",
      "tensor(129972)\n",
      "torch.Size([1, 33600, 4])\n",
      "torch.Size([1, 8, 33600])\n",
      "torch.Size([1, 68, 160, 160])\n",
      "torch.Size([1, 68, 80, 80])\n",
      "torch.Size([1, 68, 40, 40])\n",
      "\n",
      " assigner \n",
      "\n",
      "\n",
      "\n",
      " slutt loss \n",
      "\n",
      "\n",
      "torch.Size([1, 33600, 64])\n",
      "torch.Size([1, 33600, 4])\n",
      "torch.Size([1, 33600, 4])\n",
      "tensor([[[213., 119., 303., 247.],\n",
      "         [213., 119., 303., 247.],\n",
      "         [213., 119., 303., 247.],\n",
      "         ...,\n",
      "         [213., 119., 303., 247.],\n",
      "         [213., 119., 303., 247.],\n",
      "         [213., 119., 303., 247.]]])\n",
      "torch.Size([1, 33600, 4])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "\n",
      "\n",
      "\n",
      "tensor([ 0.0000, 14.3896,  3.8320], grad_fn=<CopySlices>)\n",
      "\n",
      " computed loss \n",
      "\n",
      "\n",
      " assigned req grad on loss \n",
      "\n",
      "\n",
      " backward pass \n",
      "\n",
      "\n",
      "\n",
      " odd for loop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " odd for loop \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emy016/Dropbox/Postdoc2/Kurs/NORA summer school 2023/NORAprosjekt/ultralytics/ultralytics/yolo/engine/predictor.py:140: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/Users/emy016/Dropbox/Postdoc2/Kurs/NORA summer school 2023/NORAprosjekt/ultralytics/ultralytics/yolo/engine/predictor.py:147: RuntimeWarning: invalid value encountered in cast\n",
      "  heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
      "0: 1280x1280 1 calcareous, 33 planktics, 10978.8ms\n",
      "Speed: 3.9ms preprocess, 10978.8ms inference, 1.4ms postprocess per image at shape (1, 3, 1280, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " visualize, save, write \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained model\n",
    "model = YOLO(\"forams4-1280-augmentedraw-bigtest-200epochs-yolov8x.pt\")\n",
    "\n",
    "#load example image\n",
    "im0 = Image.open(\"sampleimg-0.jpg\")\n",
    "im1 = Image.open(\"sampleimg-1.jpg\")\n",
    "im2 = Image.open(\"sampleimg-2.jpg\")\n",
    "im3 = Image.open(\"sampleimg-3.jpg\")\n",
    "\n",
    "#run prediction with visualize = True to generate heatmap\n",
    "results = model.predict(source=im3, save=False, visualize = True)  # save plotted images\n",
    "\n",
    "#The results are stored in /runs/detect/predict.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block defines some useful functions\n",
    "\n",
    "# define a function that reads the labels from the label file\n",
    "def labelreader(labpath=None):\n",
    "        labels = []\n",
    "        with open(labpath, 'r') as f:\n",
    "            for line in f:\n",
    "                #append each line to the labels as floats\n",
    "                labels.append(line.strip())\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i].split(' ')\n",
    "            for j in range(len(labels[i])):\n",
    "                labels[i][j] = float(labels[i][j])\n",
    "\n",
    "        #cast each element to float and store to tensor\n",
    "        labels = torch.tensor(labels)\n",
    "        return labels\n",
    "\n",
    "\n",
    "#This function generates an image where the CAM heatmap has been overlayed onto the original image\n",
    "def make_heatmap(image, heatmap, transparancy = 0.3, plot=False):\n",
    "    heatmapp = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    \n",
    "    imgg = image.astype(np.uint8)\n",
    "    cam_img = transparancy * heatmapp + (1-transparancy) * imgg\n",
    "    cam_img = cam_img.astype(np.uint8)\n",
    "    \n",
    "    if plot:\n",
    "        cv2.imshow(\"image\",cam_img)\n",
    "    \n",
    "    return cam_img\n",
    "\n",
    "\n",
    "\n",
    "# This function plots the heatmap and the bounding boxes on the same image\n",
    "def plot_heatmap(image, heatmap, bboxes = None, transparancy = 0.3, savename = None):\n",
    "    \n",
    "    cam_img = make_heatmap(image, heatmap, transparancy, plot=False)\n",
    "    \n",
    "    if bboxes is not None:\n",
    "        colors = [[0, 0, 255], [0, 255, 0], [255, 0, 0], [255,255,255]]\n",
    "        boxlabels = [\"sediment\", \"agglutinated\", \"calcareous\", \"planktic\"]\n",
    "        labheight = -10\n",
    "        labwidth = -7\n",
    "        imgsz = (image.shape[0],image.shape[1])\n",
    "        for i in range(bboxes.shape[0]):\n",
    "            box = bboxes[i,:]\n",
    "            cls = box[0]\n",
    "            cls = int(cls)\n",
    "            xc = float(box[1])\n",
    "            yc = float(box[2])\n",
    "            w = float(box[3])\n",
    "            h = float(box[4])\n",
    "            x1 = round((xc-0.5*w)*imgsz[0])\n",
    "            x2 = round((xc+0.5*w)*imgsz[0])\n",
    "            y1 = round((yc-0.5*h)*imgsz[1])\n",
    "            y2 = round((yc+0.5*h)*imgsz[1])\n",
    "            \n",
    "            cv2.rectangle(cam_img, (x1, y1), (x2, y2), colors[cls], 2)\n",
    "            cv2.putText(cam_img, boxlabels[cls], (x1+labwidth, y1+labheight),cv2.FONT_HERSHEY_SIMPLEX, 0.8, colors[cls], 2)\n",
    "    \n",
    "    if savename is not None:\n",
    "        cv2.imwrite(savename, cam_img)\n",
    "    cv2.imshow(\"image\",cam_img)\n",
    "    \n",
    "    return cam_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets reload the image as a cv2 image\n",
    "image = cv2.imread(\"sampleimg-3.jpg\")\n",
    "\n",
    "#give the path to the label file of the sample image\n",
    "labpath = \"sampleimg-3.txt\"\n",
    "\n",
    "#load labels into useable format\n",
    "bboxes = labelreader(labpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import the heatmap from the \n",
    "\n",
    "run = 4 #run number for the prediction\n",
    "# 0: \"\"\n",
    "# 1: 2\n",
    "# 2: 3\n",
    "# 3: 4\n",
    "\n",
    "layer = 21 #which layer to use for the heatmap\n",
    "\n",
    "resultfolder = \"./runs/detect/\"+ \"predict\" + str(run) + \"/\"\n",
    "\n",
    "#load heatmap\n",
    "heatmap = np.load(resultfolder + str(layer) + \".npy\")\n",
    "\n",
    "\n",
    "#run the function\n",
    "savename = \"sampleimg\"+str(run)+\"-heatmap.jpg\"\n",
    "cam_img = plot_heatmap(image,heatmap, bboxes, transparancy = 0.3, savename = savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39myolo_cam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m show_cam_on_image, scale_cam_image\n\u001b[1;32m      9\u001b[0m \u001b[39m#load model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39mforams4-1280-augmentedraw-bigtest-200epochs-yolov8x.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39msampleimg-0.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m#rgb_img = img.copy()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "# Eigen CAM #\n",
    "\n",
    "#from #https://github.com/rigvedrs/YOLO-V8-CAM/blob/main/YOLO%20v8n%20EigenCAM.ipynb\n",
    "\n",
    "from yolo_cam.eigen_cam import EigenCAM\n",
    "from yolo_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "\n",
    "\n",
    "#load model\n",
    "model = YOLO('forams4-1280-augmentedraw-bigtest-200epochs-yolov8x.pt')\n",
    "\n",
    "img = cv2.imread('sampleimg-0.jpg')\n",
    "#rgb_img = img.copy()\n",
    "rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow(\"img\",img)\n",
    "#plt.show()\n",
    "\n",
    "img = np.float32(img) / 255\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "#target_layers =[model.model.model[-4]]\n",
    "target_layers = [model.model.model[21]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1280, 3)\n",
      "\n",
      "\n",
      " YOLO call \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " predict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " type(self.predictor)\n",
      " \n",
      "<class 'ultralytics.yolo.v8.detect.predict.DetectionPredictor'>\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor __call__ \n",
      "\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor stream_inference \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch#\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch reqgrad\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      " autobackend forward \n",
      "\n",
      "\n",
      "b, ch, h, w \n",
      "\n",
      "1 3 1280 1280\n",
      "autobackend self.pt or self.nn_module \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "predict once\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.block.SPPF'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.head.Detect'>\n",
      "3\n",
      "\n",
      "\n",
      " autobackend forward y \n",
      "\n",
      "\n",
      "2\n",
      "list or tuple\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 1280x1280 34 Sediments, 9 planktics, 4451.6ms\n",
      "Speed: 8.5ms preprocess, 4451.6ms inference, 224.2ms postprocess per image at shape (1, 3, 1280, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " visualize, save, write \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The input image should np.float32 in the range [0, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m cam \u001b[39m=\u001b[39m EigenCAM(model, target_layers,task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mod\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m grayscale_cam \u001b[39m=\u001b[39m cam(rgb_img)[\u001b[39m0\u001b[39m, :, :]\n\u001b[0;32m---> 10\u001b[0m cam_image \u001b[39m=\u001b[39m show_cam_on_image(rgb_img, grayscale_cam, use_rgb\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     14\u001b[0m g_scale \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([grayscale_cam] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/Postdoc2/Kurs/NORA summer school 2023/NORAprosjekt/yolo_cam/utils/image.py:54\u001b[0m, in \u001b[0;36mshow_cam_on_image\u001b[0;34m(img, mask, use_rgb, colormap, image_weight)\u001b[0m\n\u001b[1;32m     51\u001b[0m heatmap \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32(heatmap) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmax(img) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe input image should np.float32 in the range [0, 1]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m image_weight \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m image_weight \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     59\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimage_weight should be in the range [0, 1].\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m            Got: \u001b[39m\u001b[39m{\u001b[39;00mimage_weight\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: The input image should np.float32 in the range [0, 1]"
     ]
    }
   ],
   "source": [
    "\n",
    "img = np.float32(img) / 255\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "#target_layers =[model.model.model[-4]]\n",
    "target_layers = [model.model.model[21]]\n",
    "\n",
    "cam = EigenCAM(model, target_layers,task='od')\n",
    "grayscale_cam = cam(rgb_img)[0, :, :]\n",
    "cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "g_scale = np.stack([grayscale_cam] * 3, axis=2)\n",
    "#plt.imshow(g_scale)\n",
    "\n",
    "im = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)\n",
    "Image.fromarray(np.hstack((im, cam_image)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

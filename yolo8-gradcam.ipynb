{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp311-cp311-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emy016/opt/anaconda3/envs/noratest/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/emy016/opt/anaconda3/envs/noratest/lib/python3.11/site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emy016/opt/anaconda3/envs/noratest/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!sudo pip3 install requests\n",
    "#!pip3 install tqdm\n",
    "#!pip3 install matplotlib\n",
    "#!pip3 install pyyaml\n",
    "#!pip3 install torchvision\n",
    "#!pip3 install scipy\n",
    "#!pip3 install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load libraries\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#plot a cv2 image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n",
      "\n",
      "\n",
      " smart_inference_mode \n",
      "\n",
      "\n",
      "\n",
      " smart_inference_mode: decorate \n",
      " torch.no_grad if False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# add modified YOLOv8 library (cloned from git: URL)\n",
    "\n",
    "sys.path.append('ultralytics')\n",
    "from ultralytics import YOLO #make sure you do not have another library with the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " predict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor init \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " setting up predictor_model, passing model=self.model \n",
      " type(self.predictor)\n",
      "<class 'ultralytics.yolo.v8.detect.predict.DetectionPredictor'>\n",
      "\n",
      " type(self.model)\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor setup model \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "\n",
      " basepredictor setup model \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " hi autobackend \n",
      "\n",
      "\n",
      "\n",
      " autobackend receive nn.module \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " print type(self.model) \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " type(self.predictor)\n",
      " \n",
      "<class 'ultralytics.yolo.v8.detect.predict.DetectionPredictor'>\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor __call__ \n",
      "\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor stream_inference \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " req grad True \n",
      "\n",
      "\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      " req grad True1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " req grad True2 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch#\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch reqgrad\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      " autobackend forward \n",
      "\n",
      "\n",
      "b, ch, h, w \n",
      "\n",
      "1 3 1280 1280\n",
      "autobackend self.pt or self.nn_module \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "predict once\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.block.SPPF'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.head.Detect'>\n",
      "3\n",
      "\n",
      "\n",
      " autobackend forward y \n",
      "\n",
      "\n",
      "2\n",
      "list or tuple\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      " targettensor \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " backprop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "v8DetectionLoss\n",
      "\n",
      "dict_keys(['task', 'mode', 'model', 'data', 'epochs', 'patience', 'batch', 'imgsz', 'save', 'save_period', 'cache', 'device', 'workers', 'project', 'name', 'exist_ok', 'pretrained', 'optimizer', 'verbose', 'seed', 'deterministic', 'single_cls', 'rect', 'cos_lr', 'close_mosaic', 'resume', 'amp', 'fraction', 'profile', 'overlap_mask', 'mask_ratio', 'dropout', 'val', 'split', 'save_json', 'save_hybrid', 'conf', 'iou', 'max_det', 'half', 'dnn', 'plots', 'source', 'show', 'save_txt', 'save_conf', 'save_crop', 'show_labels', 'show_conf', 'vid_stride', 'line_width', 'visualize', 'augment', 'agnostic_nms', 'classes', 'retina_masks', 'boxes', 'format', 'keras', 'optimize', 'int8', 'dynamic', 'simplify', 'opset', 'workspace', 'nms', 'lr0', 'lrf', 'momentum', 'weight_decay', 'warmup_epochs', 'warmup_momentum', 'warmup_bias_lr', 'box', 'cls', 'dfl', 'pose', 'kobj', 'label_smoothing', 'nbs', 'hsv_h', 'hsv_s', 'hsv_v', 'degrees', 'translate', 'scale', 'shear', 'perspective', 'flipud', 'fliplr', 'mosaic', 'mixup', 'copy_paste', 'cfg', 'v5loader', 'tracker'])\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['args', 'save_dir', 'done_warmup', 'model', 'data', 'imgsz', 'device', 'dataset', 'vid_path', 'vid_writer', 'plotted_img', 'data_path', 'source_type', 'batch', 'results', 'transforms', 'callbacks', 'stream', 'seen', 'windows'])\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'self', 'weights', 'device', 'dnn', 'data', 'fp16', 'fuse', 'verbose', 'w', 'nn_module', 'pt', 'jit', 'onnx', 'xml', 'engine', 'coreml', 'saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs', 'paddle', 'triton', 'nhwc', 'stride', 'model', 'metadata', 'cuda', 'names', '__class__'])\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'yaml', 'save', 'names', 'inplace', 'stride', 'nc', 'args', 'pt_path', 'task'])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " v8detpred2 -> type(self)\n",
      "\n",
      "\n",
      "\n",
      "pred_distri, pred_scores\n",
      "\n",
      "torch.Size([1, 68, 160, 160])\n",
      "1\n",
      "68\n",
      "64\n",
      "4\n",
      "\n",
      "\n",
      " pred_scores.shape \n",
      "\n",
      "\n",
      "torch.Size([1, 33600, 4])\n",
      "\n",
      "\n",
      " pred_scores.shape \n",
      "\n",
      "\n",
      "\n",
      " preprocess \n",
      "\n",
      "\n",
      "\n",
      " gt labels\n",
      "\n",
      "\n",
      "tensor([[[3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.],\n",
      "         [3.]]])\n",
      "torch.Size([1, 32, 1])\n",
      "tensor([[[ 375.0000,  607.5000,  435.0000,  666.5000],\n",
      "         [ 460.5000,  122.0000,  505.5000,  170.0000],\n",
      "         [ 412.0000,  209.5000,  464.0000,  256.5000],\n",
      "         [ 671.5000,   80.5000,  726.5000,  137.5000],\n",
      "         [ 684.5000,  195.5000,  741.5000,  254.5000],\n",
      "         [ 826.5000,  170.5000,  879.5000,  221.5000],\n",
      "         [ 930.0000,  321.0000,  956.0000,  351.0000],\n",
      "         [ 901.0000,  410.0000,  957.0000,  460.0000],\n",
      "         [ 722.0000,  603.0000,  778.0000,  669.0000],\n",
      "         [ 860.0000,  616.0000,  896.0000,  650.0000],\n",
      "         [ 994.0000,  598.0000, 1046.0000,  658.0000],\n",
      "         [ 863.0000,  672.0000,  919.0000,  724.0000],\n",
      "         [1026.0000,  741.5000, 1066.0000,  792.5000],\n",
      "         [ 735.5000,  761.0000,  784.5000,  807.0000],\n",
      "         [ 578.0000,  650.5000,  626.0000,  697.5000],\n",
      "         [ 358.5000,  745.5000,  405.5000,  792.5000],\n",
      "         [ 450.5000,  743.5000,  499.5000,  786.5000],\n",
      "         [ 370.0000,  837.5000,  440.0000,  904.5000],\n",
      "         [ 537.5000,  824.5000,  586.5000,  871.5000],\n",
      "         [ 456.0000,  939.0000,  500.0000,  983.0000],\n",
      "         [ 620.5000,  957.5000,  665.5000, 1008.5000],\n",
      "         [ 763.5000,  885.5000,  818.5000,  954.5000],\n",
      "         [ 443.5000, 1137.0000,  484.5000, 1179.0000],\n",
      "         [ 541.5000, 1106.0000,  572.5000, 1140.0000],\n",
      "         [ 532.5000, 1183.0000,  589.5000, 1243.0000],\n",
      "         [ 604.5000, 1169.5000,  655.5000, 1220.5000],\n",
      "         [ 727.5000, 1152.5000,  770.5000, 1199.5000],\n",
      "         [ 836.5000, 1094.5000,  885.5000, 1147.5000],\n",
      "         [ 945.0000, 1091.0000,  993.0000, 1145.0000],\n",
      "         [1031.5000, 1082.5000, 1066.5000, 1135.5000],\n",
      "         [ 924.5000,  914.0000,  977.5000,  970.0000],\n",
      "         [ 285.0000,  362.0000,  341.0000,  426.0000]]])\n",
      "torch.Size([1, 32, 4])\n",
      "\n",
      "\n",
      " gt labels\n",
      "\n",
      "\n",
      "\n",
      " bbox_decode \n",
      "\n",
      "torch.Size([1, 33600, 4])\n",
      "tensor(129972)\n",
      "torch.Size([1, 33600, 4])\n",
      "torch.Size([1, 8, 33600])\n",
      "torch.Size([1, 68, 160, 160])\n",
      "torch.Size([1, 68, 80, 80])\n",
      "torch.Size([1, 68, 40, 40])\n",
      "\n",
      " assigner \n",
      "\n",
      "\n",
      "\n",
      " slutt loss \n",
      "\n",
      "\n",
      "torch.Size([1, 33600, 64])\n",
      "torch.Size([1, 33600, 4])\n",
      "torch.Size([1, 33600, 4])\n",
      "tensor([[[375.0000, 607.5000, 435.0000, 666.5000],\n",
      "         [375.0000, 607.5000, 435.0000, 666.5000],\n",
      "         [375.0000, 607.5000, 435.0000, 666.5000],\n",
      "         ...,\n",
      "         [375.0000, 607.5000, 435.0000, 666.5000],\n",
      "         [375.0000, 607.5000, 435.0000, 666.5000],\n",
      "         [375.0000, 607.5000, 435.0000, 666.5000]]])\n",
      "torch.Size([1, 33600, 4])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "\n",
      "\n",
      "\n",
      "tensor([0.0000, 0.3527, 0.8960], grad_fn=<CopySlices>)\n",
      "\n",
      " computed loss \n",
      "\n",
      "\n",
      " assigned req grad on loss \n",
      "\n",
      "\n",
      " backward pass \n",
      "\n",
      "\n",
      "\n",
      " odd for loop \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " odd for loop \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emy016/Dropbox/Postdoc2/Kurs/noratest/noratest-root/ultralytics/ultralytics/yolo/engine/predictor.py:140: RuntimeWarning: invalid value encountered in divide\n",
      "  cam = cam / cam.max()\n",
      "/Users/emy016/Dropbox/Postdoc2/Kurs/noratest/noratest-root/ultralytics/ultralytics/yolo/engine/predictor.py:147: RuntimeWarning: invalid value encountered in cast\n",
      "  heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
      "0: 1280x1280 1 calcareous, 33 planktics, 16844.4ms\n",
      "Speed: 8.8ms preprocess, 16844.4ms inference, 5.8ms postprocess per image at shape (1, 3, 1280, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " visualize, save, write \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained model\n",
    "model = YOLO(\"forams4-1280-augmentedraw-bigtest-200epochs-yolov8x.pt\")\n",
    "\n",
    "#load example image\n",
    "im0 = Image.open(\"sampleimg-0.jpg\")\n",
    "im1 = Image.open(\"sampleimg-1.jpg\")\n",
    "im2 = Image.open(\"sampleimg-2.jpg\")\n",
    "im3 = Image.open(\"sampleimg-3.jpg\")\n",
    "\n",
    "#run prediction with visualize = True to generate heatmap\n",
    "results = model.predict(source=im3, save=False, visualize = True)  # save plotted images\n",
    "\n",
    "#The results are stored in /runs/detect/predict.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block defines some useful functions\n",
    "\n",
    "# define a function that reads the labels from the label file\n",
    "def labelreader(labpath=None):\n",
    "        labels = []\n",
    "        with open(labpath, 'r') as f:\n",
    "            for line in f:\n",
    "                #append each line to the labels as floats\n",
    "                labels.append(line.strip())\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = labels[i].split(' ')\n",
    "            for j in range(len(labels[i])):\n",
    "                labels[i][j] = float(labels[i][j])\n",
    "\n",
    "        #cast each element to float and store to tensor\n",
    "        labels = torch.tensor(labels)\n",
    "        return labels\n",
    "\n",
    "\n",
    "#This function generates an image where the CAM heatmap has been overlayed onto the original image\n",
    "def make_heatmap(image, heatmap, transparancy = 0.3, plot=False):\n",
    "    heatmapp = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    \n",
    "    imgg = image.astype(np.uint8)\n",
    "    cam_img = transparancy * heatmapp + (1-transparancy) * imgg\n",
    "    cam_img = cam_img.astype(np.uint8)\n",
    "    \n",
    "    if plot:\n",
    "        cv2.imshow(\"image\",cam_img)\n",
    "    \n",
    "    return cam_img\n",
    "\n",
    "\n",
    "\n",
    "# This function plots the heatmap and the bounding boxes on the same image\n",
    "def plot_heatmap(image, heatmap, bboxes = None, transparancy = 0.3, savename = None):\n",
    "    \n",
    "    cam_img = make_heatmap(image, heatmap, transparancy, plot=False)\n",
    "    \n",
    "    if bboxes is not None:\n",
    "        colors = [[0, 0, 255], [0, 255, 0], [255, 0, 0], [255,255,255]]\n",
    "        boxlabels = [\"sediment\", \"agglutinated\", \"calcareous\", \"planktic\"]\n",
    "        labheight = -10\n",
    "        labwidth = -7\n",
    "        imgsz = (image.shape[0],image.shape[1])\n",
    "        for i in range(bboxes.shape[0]):\n",
    "            box = bboxes[i,:]\n",
    "            cls = box[0]\n",
    "            cls = int(cls)\n",
    "            xc = float(box[1])\n",
    "            yc = float(box[2])\n",
    "            w = float(box[3])\n",
    "            h = float(box[4])\n",
    "            x1 = round((xc-0.5*w)*imgsz[0])\n",
    "            x2 = round((xc+0.5*w)*imgsz[0])\n",
    "            y1 = round((yc-0.5*h)*imgsz[1])\n",
    "            y2 = round((yc+0.5*h)*imgsz[1])\n",
    "            \n",
    "            cv2.rectangle(cam_img, (x1, y1), (x2, y2), colors[cls], 2)\n",
    "            cv2.putText(cam_img, boxlabels[cls], (x1+labwidth, y1+labheight),cv2.FONT_HERSHEY_SIMPLEX, 0.8, colors[cls], 2)\n",
    "    \n",
    "    if savename is not None:\n",
    "        cv2.imwrite(savename, cam_img)\n",
    "    cv2.imshow(\"image\",cam_img)\n",
    "    \n",
    "    return cam_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets reload the image as a cv2 image\n",
    "image = cv2.imread(\"sampleimg-3.jpg\")\n",
    "\n",
    "#give the path to the label file of the sample image\n",
    "labpath = \"sampleimg-3.txt\"\n",
    "\n",
    "#load labels into useable format\n",
    "bboxes = labelreader(labpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import the heatmap from the \n",
    "\n",
    "run = 4 #run number for the prediction\n",
    "# 0: \"\"\n",
    "# 1: 2\n",
    "# 2: 3\n",
    "# 3: 4\n",
    "\n",
    "layer = 21 #which layer to use for the heatmap\n",
    "\n",
    "resultfolder = \"./runs/detect/\"+ \"predict\" + str(run) + \"/\"\n",
    "\n",
    "#load heatmap\n",
    "heatmap = np.load(resultfolder + str(layer) + \".npy\")\n",
    "\n",
    "\n",
    "#run the function\n",
    "savename = \"sampleimg\"+str(run)+\"-heatmap.jpg\"\n",
    "cam_img = plot_heatmap(image,heatmap, bboxes, transparancy = 0.3, savename = savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39myolo_cam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m show_cam_on_image, scale_cam_image\n\u001b[1;32m      9\u001b[0m \u001b[39m#load model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39mforams4-1280-augmentedraw-bigtest-200epochs-yolov8x.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39msampleimg-0.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m#rgb_img = img.copy()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "# Eigen CAM #\n",
    "\n",
    "#from #https://github.com/rigvedrs/YOLO-V8-CAM/blob/main/YOLO%20v8n%20EigenCAM.ipynb\n",
    "\n",
    "from yolo_cam.eigen_cam import EigenCAM\n",
    "from yolo_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "\n",
    "\n",
    "#load model\n",
    "model = YOLO('forams4-1280-augmentedraw-bigtest-200epochs-yolov8x.pt')\n",
    "\n",
    "img = cv2.imread('sampleimg-0.jpg')\n",
    "#rgb_img = img.copy()\n",
    "rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow(\"img\",img)\n",
    "#plt.show()\n",
    "\n",
    "img = np.float32(img) / 255\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "#target_layers =[model.model.model[-4]]\n",
    "target_layers = [model.model.model[21]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1280, 3)\n",
      "\n",
      "\n",
      " YOLO call \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " predict\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " type(self.predictor)\n",
      " \n",
      "<class 'ultralytics.yolo.v8.detect.predict.DetectionPredictor'>\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor __call__ \n",
      "\n",
      "<class 'ultralytics.nn.autobackend.AutoBackend'>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " basepredictor stream_inference \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch#\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " batch reqgrad\n",
      "\n",
      "False\n",
      "\n",
      "\n",
      " autobackend forward \n",
      "\n",
      "\n",
      "b, ch, h, w \n",
      "\n",
      "1 3 1280 1280\n",
      "autobackend self.pt or self.nn_module \n",
      "\n",
      "<class 'ultralytics.nn.tasks.DetectionModel'>\n",
      "predict once\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.block.SPPF'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.conv.Conv'>\n",
      "<class 'ultralytics.nn.modules.conv.Concat'>\n",
      "<class 'ultralytics.nn.modules.block.C2f'>\n",
      "<class 'ultralytics.nn.modules.head.Detect'>\n",
      "3\n",
      "\n",
      "\n",
      " autobackend forward y \n",
      "\n",
      "\n",
      "2\n",
      "list or tuple\n",
      "2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 1280x1280 34 Sediments, 9 planktics, 4451.6ms\n",
      "Speed: 8.5ms preprocess, 4451.6ms inference, 224.2ms postprocess per image at shape (1, 3, 1280, 1280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " visualize, save, write \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The input image should np.float32 in the range [0, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m cam \u001b[39m=\u001b[39m EigenCAM(model, target_layers,task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mod\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m grayscale_cam \u001b[39m=\u001b[39m cam(rgb_img)[\u001b[39m0\u001b[39m, :, :]\n\u001b[0;32m---> 10\u001b[0m cam_image \u001b[39m=\u001b[39m show_cam_on_image(rgb_img, grayscale_cam, use_rgb\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     14\u001b[0m g_scale \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([grayscale_cam] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/Postdoc2/Kurs/NORA summer school 2023/NORAprosjekt/yolo_cam/utils/image.py:54\u001b[0m, in \u001b[0;36mshow_cam_on_image\u001b[0;34m(img, mask, use_rgb, colormap, image_weight)\u001b[0m\n\u001b[1;32m     51\u001b[0m heatmap \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32(heatmap) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmax(img) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe input image should np.float32 in the range [0, 1]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m image_weight \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m image_weight \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     59\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimage_weight should be in the range [0, 1].\u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m            Got: \u001b[39m\u001b[39m{\u001b[39;00mimage_weight\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: The input image should np.float32 in the range [0, 1]"
     ]
    }
   ],
   "source": [
    "\n",
    "img = np.float32(img) / 255\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "#target_layers =[model.model.model[-4]]\n",
    "target_layers = [model.model.model[21]]\n",
    "\n",
    "cam = EigenCAM(model, target_layers,task='od')\n",
    "grayscale_cam = cam(rgb_img)[0, :, :]\n",
    "cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "g_scale = np.stack([grayscale_cam] * 3, axis=2)\n",
    "#plt.imshow(g_scale)\n",
    "\n",
    "im = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)\n",
    "Image.fromarray(np.hstack((im, cam_image)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
